{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class BiLSTM(Recurrent):\\n    \\'\\'\\'Long-Short Term Memory unit - Hochreiter 1997.\\n\\n    For a step-by-step description of the algorithm, see\\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\\n\\n    # Arguments\\n        output_dim: dimension of the internal projections and the final output.\\n        init: weight initialization function.\\n            Can be the name of an existing function (str),\\n            or a Theano function (see: [initializations](../initializations.md)).\\n        inner_init: initialization function of the inner cells.\\n        forget_bias_init: initialization function for the bias of the forget gate.\\n            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\\n            recommend initializing with ones.\\n        activation: activation function.\\n            Can be the name of an existing function (str),\\n            or a Theano function (see: [activations](../activations.md)).\\n        inner_activation: activation function for the inner cells.\\n\\n    # References\\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\\n        - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\\n    \\'\\'\\'\\n    def __init__(self, output_dim,\\n                 init=\\'glorot_uniform\\', inner_init=\\'orthogonal\\',\\n                 forget_bias_init=\\'one\\', activation=\\'tanh\\',\\n                 inner_activation=\\'hard_sigmoid\\',bi=True, **kwargs):\\n        self.output_dim = output_dim\\n        self.init = initializations.get(init)\\n        self.inner_init = initializations.get(inner_init)\\n        self.forget_bias_init = initializations.get(forget_bias_init)\\n        self.activation = activations.get(activation)\\n        self.inner_activation = activations.get(inner_activation)\\n        self.bi = bi\\n        super(BiLSTM, self).__init__(**kwargs)\\n\\n    def build(self):\\n        input_shape = self.input_shape\\n        input_dim = input_shape[2]\\n        self.input_dim = input_dim\\n        self.input = K.placeholder(input_shape)\\n\\n        if self.stateful:\\n            self.reset_states()\\n        else:\\n            # initial states: 2 all-zero tensor of shape (output_dim)\\n            self.states = [None, None]\\n\\n        self.W_i = self.init((input_dim, self.output_dim))\\n        self.U_i = self.inner_init((self.output_dim, self.output_dim))\\n        self.b_i = K.zeros((self.output_dim,))\\n\\n        self.W_f = self.init((input_dim, self.output_dim))\\n        self.U_f = self.inner_init((self.output_dim, self.output_dim))\\n        self.b_f = self.forget_bias_init((self.output_dim,))\\n\\n        self.W_c = self.init((input_dim, self.output_dim))\\n        self.U_c = self.inner_init((self.output_dim, self.output_dim))\\n        self.b_c = K.zeros((self.output_dim,))\\n\\n        self.W_o = self.init((input_dim, self.output_dim))\\n        self.U_o = self.inner_init((self.output_dim, self.output_dim))\\n        self.b_o = K.zeros((self.output_dim,))\\n\\n        self.params = [self.W_i, self.U_i, self.b_i,\\n                       self.W_c, self.U_c, self.b_c,\\n                       self.W_f, self.U_f, self.b_f,\\n                       self.W_o, self.U_o, self.b_o]\\n\\n        if self.initial_weights is not None:\\n            self.set_weights(self.initial_weights)\\n            del self.initial_weights\\n\\n    def reset_states(self):\\n        assert self.stateful, \\'Layer must be stateful.\\'\\n        input_shape = self.input_shape\\n        if not input_shape[0]:\\n            raise Exception(\\'If a RNN is stateful, a complete \\' +\\n                            \\'input_shape must be provided \\' +\\n                            \\'(including batch size).\\')\\n        if hasattr(self, \\'states\\'):\\n            K.set_value(self.states[0],\\n                        np.zeros((input_shape[0], self.output_dim)))\\n            K.set_value(self.states[1],\\n                        np.zeros((input_shape[0], self.output_dim)))\\n        else:\\n            self.states = [K.zeros((input_shape[0], self.output_dim)),\\n                           K.zeros((input_shape[0], self.output_dim))]\\n            \\n    def get_output(self,train=False):\\n        \\n        self.go_backwards = False\\n        R1 = Recurrent.get_output(self,train)\\n        if not self.bi:\\n            return R1\\n        self.go_backwards = True\\n        R2 = Recurrent.get_output(self,train)\\n\\n        if self.return_sequences:\\n            R2 = R2[::,::-1,::]\\n        return R1/2 + R2 /2\\n        \\n\\n\\n    def step(self, x, states):\\n        assert len(states) == 2\\n        h_tm1 = states[0]\\n        c_tm1 = states[1]\\n\\n        x_i = K.dot(x, self.W_i) + self.b_i\\n        x_f = K.dot(x, self.W_f) + self.b_f\\n        x_c = K.dot(x, self.W_c) + self.b_c\\n        x_o = K.dot(x, self.W_o) + self.b_o\\n\\n        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\\n        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\\n        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\\n        h = o * self.activation(c)\\n        return h, [h, c]\\n\\n    def get_config(self):\\n        config = {\"output_dim\": self.output_dim,\\n                  \"init\": self.init.__name__,\\n                  \"inner_init\": self.inner_init.__name__,\\n                  \"forget_bias_init\": self.forget_bias_init.__name__,\\n                  \"activation\": self.activation.__name__,\\n                  \"inner_activation\": self.inner_activation.__name__}\\n        base_config = super(BiLSTM, self).get_config()\\n        return dict(list(base_config.items()) + list(config.items()))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers.recurrent import Recurrent\n",
    "from keras import backend as K\n",
    "from keras import activations, initializations\n",
    "from keras.layers.core import MaskedLayer\n",
    "\n",
    "if K._BACKEND == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\"class BiLSTM(Recurrent):\n",
    "    '''Long-Short Term Memory unit - Hochreiter 1997.\n",
    "\n",
    "    For a step-by-step description of the algorithm, see\n",
    "    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "    # Arguments\n",
    "        output_dim: dimension of the internal projections and the final output.\n",
    "        init: weight initialization function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [initializations](../initializations.md)).\n",
    "        inner_init: initialization function of the inner cells.\n",
    "        forget_bias_init: initialization function for the bias of the forget gate.\n",
    "            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "            recommend initializing with ones.\n",
    "        activation: activation function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [activations](../activations.md)).\n",
    "        inner_activation: activation function for the inner cells.\n",
    "\n",
    "    # References\n",
    "        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n",
    "        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "        - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "    '''\n",
    "    def __init__(self, output_dim,\n",
    "                 init='glorot_uniform', inner_init='orthogonal',\n",
    "                 forget_bias_init='one', activation='tanh',\n",
    "                 inner_activation='hard_sigmoid',bi=True, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.init = initializations.get(init)\n",
    "        self.inner_init = initializations.get(inner_init)\n",
    "        self.forget_bias_init = initializations.get(forget_bias_init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.inner_activation = activations.get(inner_activation)\n",
    "        self.bi = bi\n",
    "        super(BiLSTM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self):\n",
    "        input_shape = self.input_shape\n",
    "        input_dim = input_shape[2]\n",
    "        self.input_dim = input_dim\n",
    "        self.input = K.placeholder(input_shape)\n",
    "\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            # initial states: 2 all-zero tensor of shape (output_dim)\n",
    "            self.states = [None, None]\n",
    "\n",
    "        self.W_i = self.init((input_dim, self.output_dim))\n",
    "        self.U_i = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_i = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.W_f = self.init((input_dim, self.output_dim))\n",
    "        self.U_f = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_f = self.forget_bias_init((self.output_dim,))\n",
    "\n",
    "        self.W_c = self.init((input_dim, self.output_dim))\n",
    "        self.U_c = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_c = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.W_o = self.init((input_dim, self.output_dim))\n",
    "        self.U_o = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_o = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.params = [self.W_i, self.U_i, self.b_i,\n",
    "                       self.W_c, self.U_c, self.b_c,\n",
    "                       self.W_f, self.U_f, self.b_f,\n",
    "                       self.W_o, self.U_o, self.b_o]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def reset_states(self):\n",
    "        assert self.stateful, 'Layer must be stateful.'\n",
    "        input_shape = self.input_shape\n",
    "        if not input_shape[0]:\n",
    "            raise Exception('If a RNN is stateful, a complete ' +\n",
    "                            'input_shape must be provided ' +\n",
    "                            '(including batch size).')\n",
    "        if hasattr(self, 'states'):\n",
    "            K.set_value(self.states[0],\n",
    "                        np.zeros((input_shape[0], self.output_dim)))\n",
    "            K.set_value(self.states[1],\n",
    "                        np.zeros((input_shape[0], self.output_dim)))\n",
    "        else:\n",
    "            self.states = [K.zeros((input_shape[0], self.output_dim)),\n",
    "                           K.zeros((input_shape[0], self.output_dim))]\n",
    "            \n",
    "    def get_output(self,train=False):\n",
    "        \n",
    "        self.go_backwards = False\n",
    "        R1 = Recurrent.get_output(self,train)\n",
    "        if not self.bi:\n",
    "            return R1\n",
    "        self.go_backwards = True\n",
    "        R2 = Recurrent.get_output(self,train)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            R2 = R2[::,::-1,::]\n",
    "        return R1/2 + R2 /2\n",
    "        \n",
    "\n",
    "\n",
    "    def step(self, x, states):\n",
    "        assert len(states) == 2\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "\n",
    "        x_i = K.dot(x, self.W_i) + self.b_i\n",
    "        x_f = K.dot(x, self.W_f) + self.b_f\n",
    "        x_c = K.dot(x, self.W_c) + self.b_c\n",
    "        x_o = K.dot(x, self.W_o) + self.b_o\n",
    "\n",
    "        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\n",
    "        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\n",
    "        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\n",
    "        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"output_dim\": self.output_dim,\n",
    "                  \"init\": self.init.__name__,\n",
    "                  \"inner_init\": self.inner_init.__name__,\n",
    "                  \"forget_bias_init\": self.forget_bias_init.__name__,\n",
    "                  \"activation\": self.activation.__name__,\n",
    "                  \"inner_activation\": self.inner_activation.__name__}\n",
    "        base_config = super(BiLSTM, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTM(Recurrent):\n",
    "    '''Long-Short Term Memory unit - Hochreiter 1997.\n",
    "\n",
    "    For a step-by-step description of the algorithm, see\n",
    "    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "    # Arguments\n",
    "        output_dim: dimension of the internal projections and the final output.\n",
    "        init: weight initialization function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [initializations](../initializations.md)).\n",
    "        inner_init: initialization function of the inner cells.\n",
    "        forget_bias_init: initialization function for the bias of the forget gate.\n",
    "            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "            recommend initializing with ones.\n",
    "        activation: activation function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [activations](../activations.md)).\n",
    "        inner_activation: activation function for the inner cells.\n",
    "\n",
    "    # References\n",
    "        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n",
    "        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "        - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "    '''\n",
    "    def __init__(self, output_dim,\n",
    "                 init='glorot_uniform', inner_init='orthogonal',\n",
    "                 forget_bias_init='one', activation='tanh',\n",
    "                 inner_activation='hard_sigmoid', **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.init = initializations.get(init)\n",
    "        self.inner_init = initializations.get(inner_init)\n",
    "        self.forget_bias_init = initializations.get(forget_bias_init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.inner_activation = activations.get(inner_activation)\n",
    "        super(BiLSTM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self):\n",
    "        input_shape = self.input_shape\n",
    "        input_dim = input_shape[2]\n",
    "        self.input_dim = input_dim\n",
    "        self.input = K.placeholder(input_shape)\n",
    "\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            # initial states: 2 all-zero tensor of shape (output_dim)\n",
    "            self.states = [None, None]\n",
    "\n",
    "        self.W_i = self.init((input_dim, self.output_dim))\n",
    "        self.U_i = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_i = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.W_f = self.init((input_dim, self.output_dim))\n",
    "        self.U_f = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_f = self.forget_bias_init((self.output_dim,))\n",
    "\n",
    "        self.W_c = self.init((input_dim, self.output_dim))\n",
    "        self.U_c = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_c = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.W_o = self.init((input_dim, self.output_dim))\n",
    "        self.U_o = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_o = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n",
    "                                  self.W_c, self.U_c, self.b_c,\n",
    "                                  self.W_f, self.U_f, self.b_f,\n",
    "                                  self.W_o, self.U_o, self.b_o]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "            \n",
    "    def get_output(self,train=False):\n",
    "        \n",
    "        self.go_backwards = False\n",
    "        R1 = Recurrent.get_output(self,train)\n",
    "        #if not self.bi:\n",
    "        #    return R1\n",
    "        self.go_backwards = True\n",
    "        R2 = Recurrent.get_output(self,train)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            if K._BACKEND == 'tensorflow':\n",
    "                R2 = tf.reverse(R2,[False,True,False])\n",
    "            else:\n",
    "                R2 = R2[::,::-1,::]\n",
    "        return R1/2 + R2 /2\n",
    "\n",
    "    def reset_states(self):\n",
    "        assert self.stateful, 'Layer must be stateful.'\n",
    "        input_shape = self.input_shape\n",
    "        if not input_shape[0]:\n",
    "            raise Exception('If a RNN is stateful, a complete ' +\n",
    "                            'input_shape must be provided ' +\n",
    "                            '(including batch size).')\n",
    "        if hasattr(self, 'states'):\n",
    "            K.set_value(self.states[0],\n",
    "                        np.zeros((input_shape[0], self.output_dim)))\n",
    "            K.set_value(self.states[1],\n",
    "                        np.zeros((input_shape[0], self.output_dim)))\n",
    "        else:\n",
    "            self.states = [K.zeros((input_shape[0], self.output_dim)),\n",
    "                           K.zeros((input_shape[0], self.output_dim))]\n",
    "\n",
    "    def step(self, x, states):\n",
    "        assert len(states) == 2\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "\n",
    "        x_i = K.dot(x, self.W_i) + self.b_i\n",
    "        x_f = K.dot(x, self.W_f) + self.b_f\n",
    "        x_c = K.dot(x, self.W_c) + self.b_c\n",
    "        x_o = K.dot(x, self.W_o) + self.b_o\n",
    "\n",
    "        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\n",
    "        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\n",
    "        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\n",
    "        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"output_dim\": self.output_dim,\n",
    "                  \"init\": self.init.__name__,\n",
    "                  \"inner_init\": self.inner_init.__name__,\n",
    "                  \"forget_bias_init\": self.forget_bias_init.__name__,\n",
    "                  \"activation\": self.activation.__name__,\n",
    "                  \"inner_activation\": self.inner_activation.__name__}\n",
    "        base_config = super(BiLSTM, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from keras.models import Graph\n",
    "    from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.layers.recurrent import LSTM,GRU\n",
    "    #from keras.objectives import categorical_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "    def reverse(X):\n",
    "        return X[::,::,::-1]\n",
    "\n",
    "    def output_shape(input_shape):\n",
    "        # here input_shape includes the samples dimension\n",
    "        return input_shape  # shap\n",
    "\n",
    "\n",
    "    def sub_mean(X):\n",
    "        xdms = X.shape\n",
    "        return X.reshape(xdms[0])\n",
    "\n",
    "    #middle = 50\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(200,5))\n",
    "    #graph.add_input(name='input2', input_shape=(None,2))\n",
    "    inside = 50\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='sigmoid',input_shape=(200,5),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"l1\",input=\"input1\")\n",
    "    graph.add_output(name=\"output\",input=\"l1\")\n",
    "\n",
    "    graph.compile('adadelta', {'output':\"categorical_crossentropy\"})\n",
    "    print graph.predict({\"input1\":np.zeros((20,100,5))})[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 100, 50)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
