{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class BiLSTM(Recurrent):\\n    \\'\\'\\'Long-Short Term Memory unit - Hochreiter 1997.\\n\\n    For a step-by-step description of the algorithm, see\\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\\n\\n    # Arguments\\n        output_dim: dimension of the internal projections and the final output.\\n        init: weight initialization function.\\n            Can be the name of an existing function (str),\\n            or a Theano function (see: [initializations](../initializations.md)).\\n        inner_init: initialization function of the inner cells.\\n        forget_bias_init: initialization function for the bias of the forget gate.\\n            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\\n            recommend initializing with ones.\\n        activation: activation function.\\n            Can be the name of an existing function (str),\\n            or a Theano function (see: [activations](../activations.md)).\\n        inner_activation: activation function for the inner cells.\\n\\n    # References\\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\\n        - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\\n    \\'\\'\\'\\n    def __init__(self, output_dim,\\n                 init=\\'glorot_uniform\\', inner_init=\\'orthogonal\\',\\n                 forget_bias_init=\\'one\\', activation=\\'tanh\\',\\n                 inner_activation=\\'hard_sigmoid\\',bi=True, **kwargs):\\n        self.output_dim = output_dim\\n        self.init = initializations.get(init)\\n        self.inner_init = initializations.get(inner_init)\\n        self.forget_bias_init = initializations.get(forget_bias_init)\\n        self.activation = activations.get(activation)\\n        self.inner_activation = activations.get(inner_activation)\\n        self.bi = bi\\n        super(BiLSTM, self).__init__(**kwargs)\\n\\n    def build(self):\\n        input_shape = self.input_shape\\n        input_dim = input_shape[2]\\n        self.input_dim = input_dim\\n        self.input = K.placeholder(input_shape)\\n\\n        if self.stateful:\\n            self.reset_states()\\n        else:\\n            # initial states: 2 all-zero tensor of shape (output_dim)\\n            self.states = [None, None]\\n\\n        self.W_i = self.init((input_dim, self.output_dim))\\n        self.U_i = self.inner_init((self.output_dim, self.output_dim))\\n        self.b_i = K.zeros((self.output_dim,))\\n\\n        self.W_f = self.init((input_dim, self.output_dim))\\n        self.U_f = self.inner_init((self.output_dim, self.output_dim))\\n        self.b_f = self.forget_bias_init((self.output_dim,))\\n\\n        self.W_c = self.init((input_dim, self.output_dim))\\n        self.U_c = self.inner_init((self.output_dim, self.output_dim))\\n        self.b_c = K.zeros((self.output_dim,))\\n\\n        self.W_o = self.init((input_dim, self.output_dim))\\n        self.U_o = self.inner_init((self.output_dim, self.output_dim))\\n        self.b_o = K.zeros((self.output_dim,))\\n\\n        self.params = [self.W_i, self.U_i, self.b_i,\\n                       self.W_c, self.U_c, self.b_c,\\n                       self.W_f, self.U_f, self.b_f,\\n                       self.W_o, self.U_o, self.b_o]\\n\\n        if self.initial_weights is not None:\\n            self.set_weights(self.initial_weights)\\n            del self.initial_weights\\n\\n    def reset_states(self):\\n        assert self.stateful, \\'Layer must be stateful.\\'\\n        input_shape = self.input_shape\\n        if not input_shape[0]:\\n            raise Exception(\\'If a RNN is stateful, a complete \\' +\\n                            \\'input_shape must be provided \\' +\\n                            \\'(including batch size).\\')\\n        if hasattr(self, \\'states\\'):\\n            K.set_value(self.states[0],\\n                        np.zeros((input_shape[0], self.output_dim)))\\n            K.set_value(self.states[1],\\n                        np.zeros((input_shape[0], self.output_dim)))\\n        else:\\n            self.states = [K.zeros((input_shape[0], self.output_dim)),\\n                           K.zeros((input_shape[0], self.output_dim))]\\n            \\n    def get_output(self,train=False):\\n        \\n        self.go_backwards = False\\n        R1 = Recurrent.get_output(self,train)\\n        if not self.bi:\\n            return R1\\n        self.go_backwards = True\\n        R2 = Recurrent.get_output(self,train)\\n\\n        if self.return_sequences:\\n            R2 = R2[::,::-1,::]\\n        return R1/2 + R2 /2\\n        \\n\\n\\n    def step(self, x, states):\\n        assert len(states) == 2\\n        h_tm1 = states[0]\\n        c_tm1 = states[1]\\n\\n        x_i = K.dot(x, self.W_i) + self.b_i\\n        x_f = K.dot(x, self.W_f) + self.b_f\\n        x_c = K.dot(x, self.W_c) + self.b_c\\n        x_o = K.dot(x, self.W_o) + self.b_o\\n\\n        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\\n        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\\n        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\\n        h = o * self.activation(c)\\n        return h, [h, c]\\n\\n    def get_config(self):\\n        config = {\"output_dim\": self.output_dim,\\n                  \"init\": self.init.__name__,\\n                  \"inner_init\": self.inner_init.__name__,\\n                  \"forget_bias_init\": self.forget_bias_init.__name__,\\n                  \"activation\": self.activation.__name__,\\n                  \"inner_activation\": self.inner_activation.__name__}\\n        base_config = super(BiLSTM, self).get_config()\\n        return dict(list(base_config.items()) + list(config.items()))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers.recurrent import Recurrent\n",
    "from keras import backend as K\n",
    "from keras import activations, initializations\n",
    "\n",
    "if K._BACKEND == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\"class BiLSTM(Recurrent):\n",
    "    '''Long-Short Term Memory unit - Hochreiter 1997.\n",
    "\n",
    "    For a step-by-step description of the algorithm, see\n",
    "    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "    # Arguments\n",
    "        output_dim: dimension of the internal projections and the final output.\n",
    "        init: weight initialization function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [initializations](../initializations.md)).\n",
    "        inner_init: initialization function of the inner cells.\n",
    "        forget_bias_init: initialization function for the bias of the forget gate.\n",
    "            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "            recommend initializing with ones.\n",
    "        activation: activation function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [activations](../activations.md)).\n",
    "        inner_activation: activation function for the inner cells.\n",
    "\n",
    "    # References\n",
    "        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n",
    "        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "        - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "    '''\n",
    "    def __init__(self, output_dim,\n",
    "                 init='glorot_uniform', inner_init='orthogonal',\n",
    "                 forget_bias_init='one', activation='tanh',\n",
    "                 inner_activation='hard_sigmoid',bi=True, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.init = initializations.get(init)\n",
    "        self.inner_init = initializations.get(inner_init)\n",
    "        self.forget_bias_init = initializations.get(forget_bias_init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.inner_activation = activations.get(inner_activation)\n",
    "        self.bi = bi\n",
    "        super(BiLSTM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self):\n",
    "        input_shape = self.input_shape\n",
    "        input_dim = input_shape[2]\n",
    "        self.input_dim = input_dim\n",
    "        self.input = K.placeholder(input_shape)\n",
    "\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            # initial states: 2 all-zero tensor of shape (output_dim)\n",
    "            self.states = [None, None]\n",
    "\n",
    "        self.W_i = self.init((input_dim, self.output_dim))\n",
    "        self.U_i = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_i = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.W_f = self.init((input_dim, self.output_dim))\n",
    "        self.U_f = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_f = self.forget_bias_init((self.output_dim,))\n",
    "\n",
    "        self.W_c = self.init((input_dim, self.output_dim))\n",
    "        self.U_c = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_c = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.W_o = self.init((input_dim, self.output_dim))\n",
    "        self.U_o = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_o = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.params = [self.W_i, self.U_i, self.b_i,\n",
    "                       self.W_c, self.U_c, self.b_c,\n",
    "                       self.W_f, self.U_f, self.b_f,\n",
    "                       self.W_o, self.U_o, self.b_o]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def reset_states(self):\n",
    "        assert self.stateful, 'Layer must be stateful.'\n",
    "        input_shape = self.input_shape\n",
    "        if not input_shape[0]:\n",
    "            raise Exception('If a RNN is stateful, a complete ' +\n",
    "                            'input_shape must be provided ' +\n",
    "                            '(including batch size).')\n",
    "        if hasattr(self, 'states'):\n",
    "            K.set_value(self.states[0],\n",
    "                        np.zeros((input_shape[0], self.output_dim)))\n",
    "            K.set_value(self.states[1],\n",
    "                        np.zeros((input_shape[0], self.output_dim)))\n",
    "        else:\n",
    "            self.states = [K.zeros((input_shape[0], self.output_dim)),\n",
    "                           K.zeros((input_shape[0], self.output_dim))]\n",
    "            \n",
    "    def get_output(self,train=False):\n",
    "        \n",
    "        self.go_backwards = False\n",
    "        R1 = Recurrent.get_output(self,train)\n",
    "        if not self.bi:\n",
    "            return R1\n",
    "        self.go_backwards = True\n",
    "        R2 = Recurrent.get_output(self,train)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            R2 = R2[::,::-1,::]\n",
    "        return R1/2 + R2 /2\n",
    "        \n",
    "\n",
    "\n",
    "    def step(self, x, states):\n",
    "        assert len(states) == 2\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "\n",
    "        x_i = K.dot(x, self.W_i) + self.b_i\n",
    "        x_f = K.dot(x, self.W_f) + self.b_f\n",
    "        x_c = K.dot(x, self.W_c) + self.b_c\n",
    "        x_o = K.dot(x, self.W_o) + self.b_o\n",
    "\n",
    "        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\n",
    "        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\n",
    "        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\n",
    "        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"output_dim\": self.output_dim,\n",
    "                  \"init\": self.init.__name__,\n",
    "                  \"inner_init\": self.inner_init.__name__,\n",
    "                  \"forget_bias_init\": self.forget_bias_init.__name__,\n",
    "                  \"activation\": self.activation.__name__,\n",
    "                  \"inner_activation\": self.inner_activation.__name__}\n",
    "        base_config = super(BiLSTM, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BiLSTM(Recurrent):\n",
    "    '''Long-Short Term Memory unit - Hochreiter 1997.\n",
    "\n",
    "    For a step-by-step description of the algorithm, see\n",
    "    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "    # Arguments\n",
    "        output_dim: dimension of the internal projections and the final output.\n",
    "        init: weight initialization function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [initializations](../initializations.md)).\n",
    "        inner_init: initialization function of the inner cells.\n",
    "        forget_bias_init: initialization function for the bias of the forget gate.\n",
    "            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "            recommend initializing with ones.\n",
    "        activation: activation function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [activations](../activations.md)).\n",
    "        inner_activation: activation function for the inner cells.\n",
    "\n",
    "    # References\n",
    "        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n",
    "        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "        - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "    '''\n",
    "    def __init__(self, output_dim,\n",
    "                 init='glorot_uniform', inner_init='orthogonal',\n",
    "                 forget_bias_init='one', activation='tanh',\n",
    "                 inner_activation='hard_sigmoid',close=False, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.init = initializations.get(init)\n",
    "        self.inner_init = initializations.get(inner_init)\n",
    "        self.forget_bias_init = initializations.get(forget_bias_init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.close=close\n",
    "        self.inner_activation = activations.get(inner_activation)\n",
    "        super(BiLSTM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self):\n",
    "        input_shape = self.input_shape\n",
    "        input_dim = input_shape[2]\n",
    "        self.input_dim = input_dim\n",
    "        self.input = K.placeholder(input_shape)\n",
    "\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            # initial states: 2 all-zero tensor of shape (output_dim)\n",
    "            self.states = [None, None]\n",
    "\n",
    "        self.W_i = self.init((input_dim, self.output_dim))\n",
    "        self.U_i = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_i = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.W_f = self.init((input_dim, self.output_dim))\n",
    "        self.U_f = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_f = self.forget_bias_init((self.output_dim,))\n",
    "\n",
    "        self.W_c = self.init((input_dim, self.output_dim))\n",
    "        self.U_c = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_c = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.W_o = self.init((input_dim, self.output_dim))\n",
    "        self.U_o = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b_o = K.zeros((self.output_dim,))\n",
    "        \n",
    "        if self.close:\n",
    "            self.W_h =  self.init((self.output_dim, self.output_dim))\n",
    "            self.b_h = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n",
    "                                  self.W_c, self.U_c, self.b_c,\n",
    "                                  self.W_f, self.U_f, self.b_f,\n",
    "                                  self.W_o, self.U_o, self.b_o]\n",
    "        if self.close:\n",
    "            self.trainable_weights += [self.W_h,self.b_h]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "            \n",
    "    def get_output(self,train=False):\n",
    "        \n",
    "        self.go_backwards = False\n",
    "        R1 = Recurrent.get_output(self,train)\n",
    "\n",
    "        self.go_backwards = True\n",
    "        R2 = Recurrent.get_output(self,train)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            if K._BACKEND == 'tensorflow':\n",
    "                R2 = tf.reverse(R2,[False,True,False])\n",
    "            else:\n",
    "                R2 = R2[::,::-1,::]\n",
    "        if self.close:\n",
    "            return  K.dot(R1 + R2 ,self.W_h) + self.b_h\n",
    "        else:\n",
    "            return  R1 / 2 + R2 / 2\n",
    "\n",
    "    def reset_states(self):\n",
    "        assert self.stateful, 'Layer must be stateful.'\n",
    "        input_shape = self.input_shape\n",
    "        if not input_shape[0]:\n",
    "            raise Exception('If a RNN is stateful, a complete ' +\n",
    "                            'input_shape must be provided ' +\n",
    "                            '(including batch size).')\n",
    "        if hasattr(self, 'states'):\n",
    "            K.set_value(self.states[0],\n",
    "                        np.zeros((input_shape[0], self.output_dim)))\n",
    "            K.set_value(self.states[1],\n",
    "                        np.zeros((input_shape[0], self.output_dim)))\n",
    "        else:\n",
    "            self.states = [K.zeros((input_shape[0], self.output_dim)),\n",
    "                           K.zeros((input_shape[0], self.output_dim))]\n",
    "\n",
    "    def step(self, x, states):\n",
    "        assert len(states) == 2\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "\n",
    "        x_i = K.dot(x, self.W_i) + self.b_i\n",
    "        x_f = K.dot(x, self.W_f) + self.b_f\n",
    "        x_c = K.dot(x, self.W_c) + self.b_c\n",
    "        x_o = K.dot(x, self.W_o) + self.b_o\n",
    "\n",
    "        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\n",
    "        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\n",
    "        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\n",
    "        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"output_dim\": self.output_dim,\n",
    "                  \"init\": self.init.__name__,\n",
    "                  \"inner_init\": self.inner_init.__name__,\n",
    "                  \"forget_bias_init\": self.forget_bias_init.__name__,\n",
    "                  \"activation\": self.activation.__name__,\n",
    "                  \"inner_activation\": self.inner_activation.__name__}\n",
    "        base_config = super(BiLSTM, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "if  int(keras.__version__.split(\".\")[0]) >= 1.0 :\n",
    "    print \"v1\"\n",
    "    from keras import activations, initializations, regularizers\n",
    "    from keras.engine import Layer, InputSpec\n",
    "    from keras.layers.recurrent import time_distributed_dense\n",
    "\n",
    "\n",
    "    class BiLSTMv1(Recurrent):\n",
    "        '''Long-Short Term Memory unit - Hochreiter 1997.\n",
    "\n",
    "        For a step-by-step description of the algorithm, see\n",
    "        [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "        # Arguments\n",
    "            output_dim: dimension of the internal projections and the final output.\n",
    "            init: weight initialization function.\n",
    "                Can be the name of an existing function (str),\n",
    "                or a Theano function (see: [initializations](../initializations.md)).\n",
    "            inner_init: initialization function of the inner cells.\n",
    "            forget_bias_init: initialization function for the bias of the forget gate.\n",
    "                [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "                recommend initializing with ones.\n",
    "            activation: activation function.\n",
    "                Can be the name of an existing function (str),\n",
    "                or a Theano function (see: [activations](../activations.md)).\n",
    "            inner_activation: activation function for the inner cells.\n",
    "            W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "                (eg. L1 or L2 regularization), applied to the input weights matrices.\n",
    "            U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "                (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n",
    "            b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "                applied to the bias.\n",
    "            dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n",
    "            dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n",
    "\n",
    "        # References\n",
    "            - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n",
    "            - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "            - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "            - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
    "        '''\n",
    "        def __init__(self, output_dim,\n",
    "                     init='glorot_uniform', inner_init='orthogonal',\n",
    "                     forget_bias_init='one', activation='tanh',\n",
    "                     inner_activation='hard_sigmoid',\n",
    "                     W_regularizer=None, U_regularizer=None, b_regularizer=None,\n",
    "                     dropout_W=0., dropout_U=0.,close=False, **kwargs):\n",
    "            self.output_dim = output_dim\n",
    "            self.init = initializations.get(init)\n",
    "            self.inner_init = initializations.get(inner_init)\n",
    "            self.forget_bias_init = initializations.get(forget_bias_init)\n",
    "            self.activation = activations.get(activation)\n",
    "            self.inner_activation = activations.get(inner_activation)\n",
    "            self.W_regularizer = regularizers.get(W_regularizer)\n",
    "            self.U_regularizer = regularizers.get(U_regularizer)\n",
    "            self.b_regularizer = regularizers.get(b_regularizer)\n",
    "            self.dropout_W, self.dropout_U = dropout_W, dropout_U\n",
    "            self.close=close\n",
    "\n",
    "            if self.dropout_W or self.dropout_U:\n",
    "                self.uses_learning_phase = True\n",
    "            super(BiLSTMv1, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.input_spec = [InputSpec(shape=input_shape)]\n",
    "            input_dim = input_shape[2]\n",
    "            self.input_dim = input_dim\n",
    "\n",
    "            if self.stateful:\n",
    "                self.reset_states()\n",
    "            else:\n",
    "                # initial states: 2 all-zero tensors of shape (output_dim)\n",
    "                self.states = [None, None]\n",
    "\n",
    "            self.W_i = self.init((input_dim, self.output_dim),\n",
    "                                 name='{}_W_i'.format(self.name))\n",
    "            self.U_i = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                       name='{}_U_i'.format(self.name))\n",
    "            self.b_i = K.zeros((self.output_dim,), name='{}_b_i'.format(self.name))\n",
    "\n",
    "            self.W_f = self.init((input_dim, self.output_dim),\n",
    "                                 name='{}_W_f'.format(self.name))\n",
    "            self.U_f = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                       name='{}_U_f'.format(self.name))\n",
    "            self.b_f = self.forget_bias_init((self.output_dim,),\n",
    "                                             name='{}_b_f'.format(self.name))\n",
    "\n",
    "            self.W_c = self.init((input_dim, self.output_dim),\n",
    "                                 name='{}_W_c'.format(self.name))\n",
    "            self.U_c = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                       name='{}_U_c'.format(self.name))\n",
    "            self.b_c = K.zeros((self.output_dim,), name='{}_b_c'.format(self.name))\n",
    "\n",
    "            self.W_o = self.init((input_dim, self.output_dim),\n",
    "                                 name='{}_W_o'.format(self.name))\n",
    "            self.U_o = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                       name='{}_U_o'.format(self.name))\n",
    "            self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n",
    "\n",
    "            if self.close:\n",
    "                self.W_h =  self.init((self.output_dim, self.output_dim),\n",
    "                                      name='{}_W_h'.format(self.name))\n",
    "                self.b_h = K.zeros((self.output_dim,),\n",
    "                                   name='{}_b_h'.format(self.name))\n",
    "\n",
    "            self.regularizers = []\n",
    "            if self.W_regularizer:\n",
    "                if not self.close:\n",
    "                    self.W_regularizer.set_param(K.concatenate([self.W_i,\n",
    "                                                                self.W_f,\n",
    "                                                            self.W_c,\n",
    "                                                            self.W_o]))\n",
    "                if self.close:\n",
    "                     self.W_regularizer.set_param(K.concatenate([self.W_i,\n",
    "                                                            self.W_f,\n",
    "                                                            self.W_c,\n",
    "                                                            self.W_o,\n",
    "                                                            self.W_h]))\n",
    "\n",
    "                self.regularizers.append(self.W_regularizer)\n",
    "            if self.U_regularizer:\n",
    "                self.U_regularizer.set_param(K.concatenate([self.U_i,\n",
    "                                                            self.U_f,\n",
    "                                                            self.U_c,\n",
    "                                                            self.U_o]))\n",
    "                self.regularizers.append(self.U_regularizer)\n",
    "            if self.b_regularizer:\n",
    "                if not self.close:\n",
    "                    self.b_regularizer.set_param(K.concatenate([self.b_i,\n",
    "                                                                self.b_f,\n",
    "                                                                self.b_c,\n",
    "                                                                self.b_o]))\n",
    "                else:\n",
    "                    self.b_regularizer.set_param(K.concatenate([self.b_i,\n",
    "                                                                self.b_f,\n",
    "                                                                self.b_c,\n",
    "                                                                self.b_o,\n",
    "                                                               self.b_h]))\n",
    "                self.regularizers.append(self.b_regularizer)\n",
    "\n",
    "            self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n",
    "                                      self.W_c, self.U_c, self.b_c,\n",
    "                                      self.W_f, self.U_f, self.b_f,\n",
    "                                      self.W_o, self.U_o, self.b_o]\n",
    "            if self.close:\n",
    "                self.trainable_weights += [self.W_h,self.b_h]\n",
    "\n",
    "\n",
    "            if self.initial_weights is not None:\n",
    "                self.set_weights(self.initial_weights)\n",
    "                del self.initial_weights\n",
    "\n",
    "        def reset_states(self):\n",
    "            assert self.stateful, 'Layer must be stateful.'\n",
    "            input_shape = self.input_spec[0].shape\n",
    "            if not input_shape[0]:\n",
    "                raise Exception('If a RNN is stateful, a complete ' +\n",
    "                                'input_shape must be provided (including batch size).')\n",
    "            if hasattr(self, 'states'):\n",
    "                K.set_value(self.states[0],\n",
    "                            np.zeros((input_shape[0], self.output_dim)))\n",
    "                K.set_value(self.states[1],\n",
    "                            np.zeros((input_shape[0], self.output_dim)))\n",
    "            else:\n",
    "                self.states = [K.zeros((input_shape[0], self.output_dim)),\n",
    "                               K.zeros((input_shape[0], self.output_dim))]\n",
    "\n",
    "        def preprocess_input(self, x, train=False):\n",
    "            if self.consume_less == 'cpu':\n",
    "                if train and (0 < self.dropout_W < 1):\n",
    "                    dropout = self.dropout_W\n",
    "                else:\n",
    "                    dropout = 0\n",
    "                input_shape = self.input_spec[0].shape\n",
    "                input_dim = input_shape[2]\n",
    "                timesteps = input_shape[1]\n",
    "\n",
    "                x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n",
    "                                             input_dim, self.output_dim, timesteps)\n",
    "                x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n",
    "                                             input_dim, self.output_dim, timesteps)\n",
    "                x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n",
    "                                             input_dim, self.output_dim, timesteps)\n",
    "                x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n",
    "                                             input_dim, self.output_dim, timesteps)\n",
    "                return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "\n",
    "        def call(self,x,mask=None):\n",
    "\n",
    "            self.go_backwards = False\n",
    "            R1 = Recurrent.call(self,x,mask=mask)\n",
    "\n",
    "            self.go_backwards = True\n",
    "            R2 = Recurrent.call(self,x,mask=mask)\n",
    "\n",
    "            if self.return_sequences:\n",
    "                if K._BACKEND == 'tensorflow':\n",
    "                    R2 = tf.reverse(R2,[False,True,False])\n",
    "                else:\n",
    "                    R2 = R2[::,::-1,::]\n",
    "            if self.close:\n",
    "                return  K.dot(R1 + R2 ,self.W_h) + self.b_h\n",
    "            else:\n",
    "                return  R1 / 2 + R2 / 2\n",
    "            \n",
    "        def cell(self,x):\n",
    "            pass\n",
    "\n",
    "        def step(self, x, states):\n",
    "            h_tm1 = states[0]\n",
    "            c_tm1 = states[1]\n",
    "            B_U = states[2]\n",
    "            B_W = states[3]\n",
    "\n",
    "            if self.consume_less == 'cpu':\n",
    "                x_i = x[:, :self.output_dim]\n",
    "                x_f = x[:, self.output_dim: 2 * self.output_dim]\n",
    "                x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n",
    "                x_o = x[:, 3 * self.output_dim:]\n",
    "            else:\n",
    "                x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n",
    "                x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n",
    "                x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n",
    "                x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n",
    "\n",
    "            i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n",
    "            f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n",
    "            o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n",
    "\n",
    "            h = o * self.activation(c)\n",
    "            return h, [h, c]\n",
    "\n",
    "        def get_constants(self, x):\n",
    "            constants = []\n",
    "            if 0 < self.dropout_U < 1:\n",
    "                ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "                ones = K.concatenate([ones] * self.output_dim, 1)\n",
    "                B_U = [K.dropout(ones, self.dropout_U) for _ in range(4)]\n",
    "                constants.append(B_U)\n",
    "            else:\n",
    "                constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "\n",
    "            if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n",
    "                input_shape = self.input_spec[0].shape\n",
    "                input_dim = input_shape[-1]\n",
    "                ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "                ones = K.concatenate([ones] * input_dim, 1)\n",
    "                B_W = [K.dropout(ones, self.dropout_W) for _ in range(4)]\n",
    "                constants.append(B_W)\n",
    "            else:\n",
    "                constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "            return constants\n",
    "\n",
    "        def get_config(self):\n",
    "            config = {\"output_dim\": self.output_dim,\n",
    "                      \"init\": self.init.__name__,\n",
    "                      \"inner_init\": self.inner_init.__name__,\n",
    "                      \"forget_bias_init\": self.forget_bias_init.__name__,\n",
    "                      \"activation\": self.activation.__name__,\n",
    "                      \"inner_activation\": self.inner_activation.__name__,\n",
    "                      \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                      \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n",
    "                      \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                      \"dropout_W\": self.dropout_W,\n",
    "                      \"dropout_U\": self.dropout_U}\n",
    "            base_config = super(BiLSTMv1, self).get_config()\n",
    "            return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from keras.models import Sequential\\nfrom keras.layers import Dense,LSTM\\nmodel = Sequential()\\nmodel.add(BiLSTMv1(32, input_shape=(10, 64),return_sequences=True))\\n#model.add(MLSTM(32, input_shape=(10, 32),return_sequences=True))\\n#model.add(LSTM(32, input_shape=(10, 32),return_sequences=True))\\nmodel.compile(optimizer='rmsprop',\\n              loss='categorical_crossentropy',\\n              metrics=['accuracy'])\\nins = np.zeros((20,10,64))\\nmodel.predict(ins).shape\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM\n",
    "model = Sequential()\n",
    "model.add(BiLSTMv1(32, input_shape=(10, 64),return_sequences=True))\n",
    "#model.add(MLSTM(32, input_shape=(10, 32),return_sequences=True))\n",
    "#model.add(LSTM(32, input_shape=(10, 32),return_sequences=True))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "ins = np.zeros((20,10,64))\n",
    "model.predict(ins).shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as nps\n",
    "class BiSimpleRNN(Recurrent):\n",
    "    '''Fully-connected RNN where the output is to be fed back to input.\n",
    "\n",
    "    # Arguments\n",
    "        output_dim: dimension of the internal projections and the final output.\n",
    "        init: weight initialization function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [initializations](../initializations.md)).\n",
    "        inner_init: initialization function of the inner cells.\n",
    "        activation: activation function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [activations](../activations.md)).\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the input weights matrices.\n",
    "        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n",
    "        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n",
    "\n",
    "    # References\n",
    "        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
    "    '''\n",
    "    def __init__(self, output_dim,\n",
    "                 init='glorot_uniform', inner_init='orthogonal',\n",
    "                 activation='tanh',\n",
    "                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n",
    "                 dropout_W=0., dropout_U=0.,close=False, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.init = initializations.get(init)\n",
    "        self.inner_init = initializations.get(inner_init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.W_regularizer = W_regularizer\n",
    "        self.U_regularizer = U_regularizer\n",
    "        self.b_regularizer = b_regularizer\n",
    "        self.close = close\n",
    "        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n",
    "        super(BiSimpleRNN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self):\n",
    "        input_shape = self.input_shape\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            # initial states: all-zero tensor of shape (output_dim)\n",
    "            self.states = [None]\n",
    "        input_dim = input_shape[2]\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W = self.init((input_dim, self.output_dim))\n",
    "        self.U = self.inner_init((self.output_dim, self.output_dim))\n",
    "        self.b = K.zeros((self.output_dim,))\n",
    "        \n",
    "\n",
    "        def append_regulariser(input_regulariser, param, regularizers_list):\n",
    "            regulariser = regularizers.get(input_regulariser)\n",
    "            if regulariser:\n",
    "                regulariser.set_param(param)\n",
    "                regularizers_list.append(regulariser)\n",
    "\n",
    "      \n",
    "        \n",
    "        if self.close:\n",
    "            self.W_h =  self.init((self.output_dim, self.output_dim))\n",
    "            self.b_h = K.zeros((self.output_dim,))\n",
    "\n",
    "        self.trainable_weights = [self.W, self.U, self.b]\n",
    "        \n",
    "        if self.close:\n",
    "            self.trainable_weights += [self.W_h,self.b_h]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "    \n",
    "    def get_output(self,train=False):\n",
    "        \n",
    "        self.go_backwards = False\n",
    "        R1 = Recurrent.get_output(self,train)\n",
    "\n",
    "        self.go_backwards = True\n",
    "        R2 = Recurrent.get_output(self,train)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            if K._BACKEND == 'tensorflow':\n",
    "                R2 = tf.reverse(R2,[False,True,False])\n",
    "            else:\n",
    "                R2 = R2[::,::-1,::]\n",
    "        if self.close:\n",
    "            return  K.dot(R1 + R2 ,self.W_h) + self.b_h\n",
    "        else:\n",
    "            return  R1 / 2 + R2 / 2\n",
    "\n",
    "    def reset_states(self):\n",
    "        assert self.stateful, 'Layer must be stateful.'\n",
    "        input_shape = self.input_shape\n",
    "        if not input_shape[0]:\n",
    "            raise Exception('If a RNN is stateful, a complete ' +\n",
    "                            'input_shape must be provided (including batch size).')\n",
    "        if hasattr(self, 'states'):\n",
    "            K.set_value(self.states[0],\n",
    "                        np.zeros((input_shape[0], self.output_dim)))\n",
    "        else:\n",
    "            self.states = [K.zeros((input_shape[0], self.output_dim))]\n",
    "\n",
    "    def step(self, x, states):\n",
    "        # states contains the previous output,\n",
    "        # and the two dropout matrices from self.get_constants()\n",
    "        assert len(states) == 1  # 1 state and 2 constants\n",
    "        prev_output = states[0]\n",
    "      \n",
    "        h = K.dot(x,  self.W) + self.b\n",
    "        output = self.activation(h + K.dot(prev_output , self.U))\n",
    "        return output, [output]\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"output_dim\": self.output_dim,\n",
    "                  \"init\": self.init.__name__,\n",
    "                  \"inner_init\": self.inner_init.__name__,\n",
    "                  \"activation\": self.activation.__name__,\n",
    "                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n",
    "                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                  \"dropout_W\": self.dropout_W,\n",
    "                  \"dropout_U\": self.dropout_U}\n",
    "        base_config = super(SimpleRNN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "if  int(keras.__version__.split(\".\")[0]) >= 1.0 :\n",
    "    print \"v1\"\n",
    "    from keras import activations, initializations, regularizers\n",
    "    from keras.engine import Layer, InputSpec\n",
    "    from keras.layers.recurrent import time_distributed_dense,Recurrent\n",
    "    \n",
    "    \n",
    "    class BiSimpleRNNv1(Recurrent):\n",
    "        '''Fully-connected RNN where the output is to be fed back to input.\n",
    "\n",
    "        # Arguments\n",
    "            output_dim: dimension of the internal projections and the final output.\n",
    "            init: weight initialization function.\n",
    "                Can be the name of an existing function (str),\n",
    "                or a Theano function (see: [initializations](../initializations.md)).\n",
    "            inner_init: initialization function of the inner cells.\n",
    "            activation: activation function.\n",
    "                Can be the name of an existing function (str),\n",
    "                or a Theano function (see: [activations](../activations.md)).\n",
    "            W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "                (eg. L1 or L2 regularization), applied to the input weights matrices.\n",
    "            U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "                (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n",
    "            b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "                applied to the bias.\n",
    "            dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n",
    "            dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n",
    "\n",
    "        # References\n",
    "            - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
    "        '''\n",
    "        def __init__(self, output_dim,\n",
    "                     init='glorot_uniform', inner_init='orthogonal',\n",
    "                     activation='tanh',close = True,\n",
    "                     W_regularizer=None, U_regularizer=None, b_regularizer=None,\n",
    "                     dropout_W=0., dropout_U=0., **kwargs):\n",
    "            self.output_dim = output_dim\n",
    "            self.init = initializations.get(init)\n",
    "            self.inner_init = initializations.get(inner_init)\n",
    "            self.activation = activations.get(activation)\n",
    "            self.W_regularizer = regularizers.get(W_regularizer)\n",
    "            self.U_regularizer = regularizers.get(U_regularizer)\n",
    "            self.b_regularizer = regularizers.get(b_regularizer)\n",
    "            self.close = close\n",
    "            self.dropout_W, self.dropout_U = dropout_W, dropout_U\n",
    "\n",
    "            if self.dropout_W or self.dropout_U:\n",
    "                self.uses_learning_phase = True\n",
    "            super(BiSimpleRNNv1, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.input_spec = [InputSpec(shape=input_shape)]\n",
    "            if self.stateful:\n",
    "                self.reset_states()\n",
    "            else:\n",
    "                # initial states: all-zero tensor of shape (output_dim)\n",
    "                self.states = [None]\n",
    "            input_dim = input_shape[2]\n",
    "            self.input_dim = input_dim\n",
    "\n",
    "            self.W = self.init((input_dim, self.output_dim),\n",
    "                               name='{}_W'.format(self.name))\n",
    "            self.U = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                     name='{}_U'.format(self.name))\n",
    "            self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n",
    "\n",
    "            if self.close:\n",
    "                self.W_h =  self.init((self.output_dim, self.output_dim),\n",
    "                                      name='{}_W_h'.format(self.name))\n",
    "                self.b_h = K.zeros((self.output_dim,),\n",
    "                                   name='{}_b_h'.format(self.name))\n",
    "\n",
    "            self.regularizers = []\n",
    "            if self.W_regularizer:\n",
    "                if self.close:\n",
    "                    self.W_regularizer.set_param(K.concatenate([self.W,self.W_h]))\n",
    "                else:\n",
    "                    self.W_regularizer.set_param(self.W)\n",
    "\n",
    "                                   \n",
    "                self.regularizers.append(self.W_regularizer)\n",
    "\n",
    "            if self.U_regularizer:\n",
    "                self.U_regularizer.set_param(self.U)\n",
    "                self.regularizers.append(self.U_regularizer)\n",
    "            if self.b_regularizer:\n",
    "                if self.close:\n",
    "                    self.b_regularizer.set_param(K.concatenate([self.b,self.b_h]))\n",
    "                else:\n",
    "                    self.b_regularizer.set_param(self.b)\n",
    "\n",
    "                self.regularizers.append(self.b_regularizer)\n",
    "\n",
    "            self.trainable_weights = [self.W, self.U, self.b]\n",
    "            \n",
    "            if self.close:\n",
    "                self.trainable_weights += [self.W_h,self.b_h]\n",
    "\n",
    "            if self.initial_weights is not None:\n",
    "                self.set_weights(self.initial_weights)\n",
    "                del self.initial_weights\n",
    "\n",
    "        def reset_states(self):\n",
    "            assert self.stateful, 'Layer must be stateful.'\n",
    "            input_shape = self.input_spec[0].shape\n",
    "            if not input_shape[0]:\n",
    "                raise Exception('If a RNN is stateful, a complete ' +\n",
    "                                'input_shape must be provided (including batch size).')\n",
    "            if hasattr(self, 'states'):\n",
    "                K.set_value(self.states[0],\n",
    "                            np.zeros((input_shape[0], self.output_dim)))\n",
    "            else:\n",
    "                self.states = [K.zeros((input_shape[0], self.output_dim))]\n",
    "\n",
    "        def preprocess_input(self, x):\n",
    "            if self.consume_less == 'cpu':\n",
    "                input_shape = self.input_spec[0].shape\n",
    "                input_dim = input_shape[2]\n",
    "                timesteps = input_shape[1]\n",
    "                return time_distributed_dense(x, self.W, self.b, self.dropout_W,\n",
    "                                              input_dim, self.output_dim,\n",
    "                                              timesteps)\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        def call(self,x,mask=None):\n",
    "\n",
    "            self.go_backwards = False\n",
    "            R1 = Recurrent.call(self,x,mask=mask)\n",
    "\n",
    "            self.go_backwards = True\n",
    "            R2 = Recurrent.call(self,x,mask=mask)\n",
    "\n",
    "            if self.return_sequences:\n",
    "                if K._BACKEND == 'tensorflow':\n",
    "                    R2 = tf.reverse(R2,[False,True,False])\n",
    "                else:\n",
    "                    R2 = R2[::,::-1,::]\n",
    "            if self.close:\n",
    "                return  K.dot(R1 + R2 ,self.W_h) + self.b_h\n",
    "            else:\n",
    "                return  R1 / 2 + R2 / 2\n",
    "\n",
    "        def step(self, x, states):\n",
    "            prev_output = states[0]\n",
    "            B_U = states[1]\n",
    "            B_W = states[2]\n",
    "\n",
    "            if self.consume_less == 'cpu':\n",
    "                h = x\n",
    "            else:\n",
    "                h = K.dot(x * B_W, self.W) + self.b\n",
    "\n",
    "            output = self.activation(h + K.dot(prev_output * B_U, self.U))\n",
    "            return output, [output]\n",
    "\n",
    "        def get_constants(self, x):\n",
    "            constants = []\n",
    "            if 0 < self.dropout_U < 1:\n",
    "                ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "                ones = K.concatenate([ones] * self.output_dim, 1)\n",
    "                B_U = K.in_train_phase(K.dropout(ones, self.dropout_U), ones)\n",
    "                constants.append(B_U)\n",
    "            else:\n",
    "                constants.append(K.cast_to_floatx(1.))\n",
    "            if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n",
    "                input_shape = self.input_spec[0].shape\n",
    "                input_dim = input_shape[-1]\n",
    "                ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "                ones = K.concatenate([ones] * input_dim, 1)\n",
    "                B_W = K.in_train_phase(K.dropout(ones, self.dropout_W), ones)\n",
    "                constants.append(B_W)\n",
    "            else:\n",
    "                constants.append(K.cast_to_floatx(1.))\n",
    "            return constants\n",
    "\n",
    "        def get_config(self):\n",
    "            config = {'output_dim': self.output_dim,\n",
    "                      'init': self.init.__name__,\n",
    "                      'inner_init': self.inner_init.__name__,\n",
    "                      'activation': self.activation.__name__,\n",
    "                      'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                      'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n",
    "                      'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                      'dropout_W': self.dropout_W,\n",
    "                      'dropout_U': self.dropout_U}\n",
    "            base_config = super(BiSimpleRNNv1, self).get_config()\n",
    "            return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 100, 50)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from keras.models import Graph\n",
    "    from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.layers.recurrent import LSTM,GRU\n",
    "    #from keras.objectives import categorical_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "    def reverse(X):\n",
    "        return X[::,::,::-1]\n",
    "\n",
    "    def output_shape(input_shape):\n",
    "        # here input_shape includes the samples dimension\n",
    "        return input_shape  # shap\n",
    "\n",
    "\n",
    "    def sub_mean(X):\n",
    "        xdms = X.shape\n",
    "        return X.reshape(xdms[0])\n",
    "\n",
    "    #middle = 50\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(None,5))\n",
    "    #graph.add_input(name='input2', input_shape=(None,2))\n",
    "    inside = 50\n",
    "    #graph.add_node(BiLSTM(output_dim=inside, activation='sigmoid',input_shape=(200,5),\n",
    "    #                    inner_activation='hard_sigmoid',return_sequences=True),\n",
    "    #                   name=\"l1\",input=\"input1\")\n",
    "    graph.add_node(BiSimpleRNNv1(output_dim=inside, activation='sigmoid',input_shape=(None,5),return_sequences=True),\n",
    "                       name=\"l1\",input=\"input1\")\n",
    "    graph.add_output(name=\"output\",input=\"l1\")\n",
    "\n",
    "    graph.compile('adadelta', {'output':\"categorical_crossentropy\"})\n",
    "    print graph.predict({\"input1\":np.zeros((20,100,5))})[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
